{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Progetto di Analisi Immagini e Video:\n",
    "\n",
    "# Generazione di Volti con il Diffusion Model\n",
    "\n",
    "Molinaro Pasquale 235309, Centraco Giuseppe 227591"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Indice\n",
    "\n",
    "1. [Introduzione](#introduzione)\n",
    "    1.1. [Librerie utilizzate](#librerie-utilizzate)\n",
    "    1.2. [Configurazione ambiente](#configurazione-ambiente)\n",
    "2. [Data Augmentation](#data-augmentation)\n",
    "3. [Classe EMA](#classe-ema)\n",
    "4. [Calcolo del passo di diffusione](#calcolo-del-passo-di-diffusione)\n",
    "5. [Metodi ausiliari per la creazione della rete](#metodi-ausialiari-per-la-creazione-della-rete)\n",
    "6. [Rete convoluzionale](#rete-convoluzionale)\n",
    "7. [Training Loop](#training-loop)\n",
    "8. [Main](#main)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduzione <a id=\"introduzione\"></a>\n",
    "\n",
    "Il Diffusion Model è un potente algoritmo di deep learning che consente di generare immagini di alta qualità utilizzando una tecnica di diffusione stocastica. La generazione di volti realistici è una sfida affascinante nell'ambito dell'intelligenza artificiale. Esso si basa su un'architettura neurale generativa avanzata, in grado di apprendere e riprodurre le caratteristiche distintive dei volti umani con notevole fedeltà.\n",
    "\n",
    "In questo notebook, esploreremo il processo di generazione di volti passo dopo passo.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Librerie utilizzate <a id=\"librerie-utilizzate\"></a>\n",
    "\n",
    "Inizieremo importando le librerie necessarie e preparando l'ambiente di lavoro. Successivamente, analizzeremo il dataset utilizzato per addestrare il modello e lo prenderemo in considerazione per comprendere meglio le caratteristiche dei volti umani. Proseguiremo quindi con la creazione e l'addestramento del Diffusion Model, che utilizzeremo infine per generare nuovi volti."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorboard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m  SummaryWriter\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:1\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorboard\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdistutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LooseVersion\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(tensorboard, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m LooseVersion(\n\u001B[0;32m      5\u001B[0m     tensorboard\u001B[38;5;241m.\u001B[39m__version__\n\u001B[0;32m      6\u001B[0m ) \u001B[38;5;241m<\u001B[39m LooseVersion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.15\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from torch.utils.tensorboard import  SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.functional\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from torch import optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-25T17:12:28.939487300Z",
     "start_time": "2023-06-25T17:12:27.642751800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configurazione ambiente <a id=\"configurazione-ambiente\"></a>\n",
    "\n",
    "Tramite la libreria torch.device viene impostata la scheda video come motore di esecuzione principale per il training della rete."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Impostazione del seed per la riproducibilità\n",
    "seed = 33\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "train_cond = True\n",
    "batch_size =90\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:48:26.838933Z",
     "iopub.execute_input": "2023-06-22T11:48:26.839295Z",
     "iopub.status.idle": "2023-06-22T11:48:26.876348Z",
     "shell.execute_reply.started": "2023-06-22T11:48:26.839262Z",
     "shell.execute_reply": "2023-06-22T11:48:26.875367Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data augmentation <a id=\"data-augmentation\"></a>\n",
    "\n",
    "Attraverso la funzione transform vengono applicate delle trasformazioni ad ogni immagine che viene presa in input. Le trasformazioni servono per fare data augmentation, così da poter rendere più facile al modello la creazione di immagini più realistiche.\n",
    "In particolare vengono applicate:\n",
    "  - una resize da 64x64 a 80x80;\n",
    "  - una randomResizedCrop con una dimensione originale di 64x64;\n",
    "  - un filtro Gaussiano così da permettere alla rete di imapare a riconoscere meglio i volti. Questo potrebbe accadere perché, facendo così, il volto in primo piano risulta più in vista rispetto allo sfondo che apparirebbe più sfuocato;\n",
    "  - un filtro Gaussiano c;\n",
    "  - una randomFlip dell'immagine;\n",
    "  - cambiamento nell'intensità e nella saturazione dei pixel all'interno dell'immagine.\n",
    "\n",
    "In seguito, grazie ad un dataloader, viene caricato il dataset di immagini con l'applicazione della transformazione."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#uso la trasformazione con i valori di media e deviazione standard tipici della normalizzazione per foto rgb usati nella ResNe\n",
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n",
    "    torchvision.transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
    "    transforms.RandomChoice([\n",
    "    transforms.Lambda(lambda x: torchvision.transforms.GaussianBlur(kernel_size=3, sigma=(0.01, 0.25))(x) if torch.rand(1) < 0.5 else x),]),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ], p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomApply([\n",
    "            transforms.Lambda(lambda x: torchvision.transforms.functional.adjust_hue(x, 0.1)),\n",
    "            transforms.Lambda(lambda x: torchvision.transforms.functional.adjust_saturation(x, 0.1)),\n",
    "        ], p=0.2),\n",
    "    ], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "db = \"/kaggle/input/dataset-2-label/dataset_2_label\"\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(db, transform=transform)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:48:26.879612Z",
     "iopub.execute_input": "2023-06-22T11:48:26.881867Z",
     "iopub.status.idle": "2023-06-22T11:49:08.207293Z",
     "shell.execute_reply.started": "2023-06-22T11:48:26.881827Z",
     "shell.execute_reply": "2023-06-22T11:49:08.206322Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classe EMA <a id=\"classe-ema\"></a>\n",
    "\n",
    "La classe \"EMA\" nel Diffusion Model si riferisce alla Media Mobile Esponenziale. Si tratta di una tecnica utilizzata per calcolare una media ponderata dei dati nel tempo, sostanzialmente viene assegnato un peso maggiore ai dati più recenti.\n",
    "\n",
    "Permette di mantenere una media mobile dei parametri del modello nel corso dell'addestramento, fornendo una stima più stabile e affidabile dei parametri ottimali. Ciò può aiutare a ridurre l'instabilità durante l'addestramento e migliorare le prestazioni del modello finale. Può essere usata per stabilizzare il processo di generazione facciale producendo immagini realistiche e coerenti nel tempo. Aiuta a ridurre le fluttuazioni indesiderate.\n",
    "\n",
    "L'EMA è calcolata tramite la seguente formula:\n",
    "\n",
    "**EMA(t) = (1 - alpha) * EMA(t-1) + alpha * x(t)**\n",
    "\n",
    "Dove:\n",
    "\n",
    "- EMA(t) rappresenta il valore dell'EMA al tempo t;\n",
    "- alpha è un fattore di smoothing che determina il peso relativo dei dati passati rispetto ai nuovi dati. Solitamente, alpha è un valore compreso tra 0 e 1, dove valori più alti danno maggior peso ai dati più recenti;\n",
    "- EMA(t-1) è il valore dell'EMA al tempo precedente t-1;\n",
    "- x(t) è il dato corrente al tempo t.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class EMA:\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.alpha + (1 - self.alpha) * new\n",
    "\n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "\n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.210117Z",
     "iopub.execute_input": "2023-06-22T11:49:08.210565Z",
     "iopub.status.idle": "2023-06-22T11:49:08.220001Z",
     "shell.execute_reply.started": "2023-06-22T11:49:08.210531Z",
     "shell.execute_reply": "2023-06-22T11:49:08.219119Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calcolo del passo di diffusione <a id=\"calcolo-del-passo-di-diffusione\"></a>\n",
    "\n",
    "In questo codice viene definita una classe chiamata Diffusion. Questa classe implementa una procedura di diffusione per generare nuove immagini a partire da un modello.\n",
    "\n",
    "Nel metodo __init__, vengono inizializzati diversi attributi della classe, tra cui noise_steps (numero di passi di rumore), beta_start e beta_end (i valori di inizio e fine per la scheda del rumore), img_size (dimensione delle immagini), e device (dispositivo su cui eseguire il calcolo, di default \"cuda\" che indica la GPU).\n",
    "\n",
    "Viene quindi chiamato il metodo prepare_noise_schedule per generare una sequenza di valori di rumore beta distribuiti linearmente tra beta_start e beta_end. Questa sequenza viene spostata sul dispositivo specificato (self.device), e vengono calcolati anche alpha (1 - beta) e alpha_hat che è il cumprod (cumulative product) di alpha.\n",
    "\n",
    "Il metodo prepare_noise_schedule restituisce una sequenza di valori di rumore beta generati tramite torch.linspace tra beta_start e beta_end con dimensione noise_steps.\n",
    "\n",
    "Il metodo noise_images prende un tensore x e un indice t, e restituisce un nuovo tensore di rumore generato a partire da x e un rumore casuale Ɛ. Questo avviene calcolando le radici quadrate di alpha_hat e 1 - alpha_hat e moltiplicandole rispettivamente per x e Ɛ. Questo viene fatto per ottenere un contributo di rumore graduale durante il processo di diffusione.\n",
    "\n",
    "Il metodo sample_timesteps genera un tensore di indici casuali t con dimensione n compresi tra 1 e noise_steps.\n",
    "\n",
    "Il metodo sample esegue il campionamento di nuove immagini utilizzando il modello specificato. Viene eseguito in modalità di valutazione (model.eval()) e senza calcoli di gradiente (torch.no_grad()). Viene inizializzato un tensore x di rumore casuale. Successivamente, viene iterato all'indietro sui passi di rumore (reversed(range(1, self.noise_steps))) e per ogni passo viene generato il rumore previsto dal modello (predicted_noise) applicato a x. Se cfg_scale è maggiore di 0, viene calcolato anche il rumore previsto in assenza di condizioni (uncond_predicted_noise) e viene eseguita una combinazione lineare tra i due rumori (predicted_noise) in base al valore di cfg_scale. Vengono calcolati i fattori di scala alpha, alpha_hat e beta, e viene generato un rumore casuale noise. Infine, viene applicata la formula di diffusione per aggiornare il tensore x. Alla fine del campionamento, il modello viene riportato in modalità di allenamento (model.train()). Vengono quindi applicate alcune operazioni di normalizzazione e conversione dei valori di x per ottenere un tensore di immagini finale (x).\n",
    "\n",
    "La formula è composta da\n",
    "* $\\alpha_t = 1 - \\beta$\n",
    "e da\n",
    "* $\\bar{\\alpha_t} = \\Pi^{t}_{s=1} a_s$\n",
    "calcolo il primo **semplicemente assegnandolo ad una variabile**, mentre calcolo il secondo usando la **funzione linspace** per calcolare la produttoria.\n",
    "\n",
    "Vado in seguito a calcolare la formula per il passo di diffusione t restituendolo nella **return**\n",
    "* $x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t}\\epsilon$\n",
    "\n",
    "Eseguo poi il cilo di sampling:\n",
    "\n",
    "* for $t = T....1$ do\n",
    "  z - $N(0,1)$ if > 1, else z=0\n",
    "  $x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}}_t}\\epsilon_g(x_t,t)) + \\sigma_t z$\n",
    "  end for\n",
    "  return $x_0$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=400, beta_start=1e-4, beta_end=0.02, img_size=64, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, cfg_scale=3):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t, labels)\n",
    "                if cfg_scale > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.222193Z",
     "iopub.execute_input": "2023-06-22T11:49:08.222834Z",
     "iopub.status.idle": "2023-06-22T11:49:08.239202Z",
     "shell.execute_reply.started": "2023-06-22T11:49:08.222802Z",
     "shell.execute_reply": "2023-06-22T11:49:08.238258Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metodi ausialiari per la creazione della rete <a id=\"metodi-ausiliari-per-la-creazione-della-rete\"></a>\n",
    "\n",
    "Il metodo __init__ inizializza la classe DoubleConv. Prende diversi argomenti: in_channels (numero dei canali di input), out_channels (numero dei canali di output), mid_channels (numero dei canali intermedi, che di default è uguale a out_channels se non viene fornito), e residual (un flag booleano che indica se utilizzare le connessioni residue, che di default è False).\n",
    "\n",
    "All'interno del metodo __init__, il flag residual viene assegnato all'attributo self.residual per essere utilizzato successivamente nel metodo forward.\n",
    "\n",
    "L'attributo double_conv viene definito come un contenitore sequenziale di moduli PyTorch. Consiste in due strati convoluzionali con dimensione del kernel 3 e padding 1, seguiti da normalizzazione di gruppo e attivazione GELU. Il primo strato convoluzionale prende in_channels come input e produce mid_channels come output, mentre il secondo strato convoluzionale prende mid_channels come input e produce out_channels come output. La normalizzazione di gruppo viene applicata dopo ogni strato convoluzionale, con una dimensione di gruppo di 1, il che significa che la normalizzazione viene eseguita indipendentemente per ogni canale.\n",
    "\n",
    "Nel passo di forward, qualora il flag di blocco residuale sia attivo, viene attivato il passo residuale stesso, altrimenti no."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.240824Z",
     "iopub.execute_input": "2023-06-22T11:49:08.241222Z",
     "iopub.status.idle": "2023-06-22T11:49:08.254486Z",
     "shell.execute_reply.started": "2023-06-22T11:49:08.241191Z",
     "shell.execute_reply": "2023-06-22T11:49:08.253427Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Il modulo SelfAttention implementa un meccanismo di self-attention per elaborare un tensore di input. Applica la self-attention per catturare le relazioni tra le diverse posizioni nel tensore di input e modifica l'input originale utilizzando un modulo di feed-forward per ottenere un output arricchito con informazioni di attenzione.\n",
    "\n",
    "Nel metodo __init__, vengono inizializzati gli attributi della classe, tra cui channels (numero di canali del tensore di input) e size (dimensione del lato del tensore di input). Viene creato un modulo di multihead attention (self.mha) utilizzando la classe nn.MultiheadAttention. Questo modulo prende in input il numero di canali, il numero di testine di attenzione (impostato a 4) e l'opzione batch_first impostata su True per specificare che la dimensione del batch è la prima dimensione del tensore di input. Viene inoltre creato un modulo di layer normalization (self.ln) utilizzando la classe nn.LayerNorm con una dimensione di input uguale a [channels] per normalizzare i valori lungo la dimensione dei canali.\n",
    "\n",
    "Il modulo ff_self è definito come un sequenziale di operazioni. Comprende un modulo di layer normalization (nn.LayerNorm) con una dimensione di input uguale a [channels], seguito da due moduli lineari (nn.Linear) con la stessa dimensione di input e output channels, separati da una funzione di attivazione GELU (nn.GELU)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.256088Z",
     "iopub.execute_input": "2023-06-22T11:49:08.256435Z",
     "iopub.status.idle": "2023-06-22T11:49:08.266352Z",
     "shell.execute_reply.started": "2023-06-22T11:49:08.256404Z",
     "shell.execute_reply": "2023-06-22T11:49:08.265410Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_images(images, num_samples):\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow(torch.cat([\n",
    "        torch.cat([i for i in images[:num_samples].to(device)], dim=-1),], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.300309Z",
     "iopub.execute_input": "2023-06-22T11:49:08.300634Z",
     "iopub.status.idle": "2023-06-22T11:49:08.311658Z",
     "shell.execute_reply.started": "2023-06-22T11:49:08.300594Z",
     "shell.execute_reply": "2023-06-22T11:49:08.310667Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rete Convoluzionale <a id=\"rete-convoluzionale\"></a>\n",
    "\n",
    "La classe \"MyUNetConditioned\" è un modulo di rete neurale convoluzionale che implementa un'architettura di rete. Essa è una versione personalizzata di una rete UNet e viene utilizzata per la segmentazione di immagini. Essa presenta 4 blocchi convoluzioniali di discesa, un passo di bottleneck e altri 4 passi convoluzionali di salita inframezzati dal calcolo di layer di selfAttention per poter marcare meglio quelle che sono le caratteristiche che spiccano di più di ogni singola foto.\n",
    "Abbiamo due variabili, c?in e c?out che indicano i canali in entrata e i canali in uscita della rete. Si parte quindi da un numero di canali pari a 3 per arrivare, all'interno del passo di bottleneck ad un numero di canali di 512. La prima chiamata alla classe DoubleConv contiene semmpre il flag **residual=True**, questo sta ad indicare che per quello specifico blocco convoluzionale è viene usata una rete residuale. Durante il passo di discesa viene utilizzata una funzione \"MaxPool2d\". La funzione nn.MaxPool2d è un modulo di PyTorch che implementa l'operazione di max pooling su un input bidimensionale. Il max pooling riduce la dimensione spaziale di un tensore di input mantenendo il valore massimo all'interno di ciascuna finestra di pooling.\n",
    "Nel passo di risalita invece viene utilizzata la funzione Upsample. La funzione nn.Upsample è un modulo di PyTorch che implementa l'operazione di upsampling (aumento della risoluzione) su un input. L'upsampling aumenta le dimensioni spaziali di un tensore interpolando i valori esistenti.\n",
    "\n",
    "Viene in seguito, definita la funzione di **pos_encoding**. Il metodo pos_encoding definisce la codifica di posizione utilizzata nella classe MyUNetConditioned. Questa codifica di posizione viene applicata al tempo t per incorporare l'informazione temporale nel modello. Questa codifica di posizione basata sul tempo t utilizza il seno e coseno. Essa viene successivamente utilizzata nel metodo forward per arricchire l'input del modello con informazioni temporali.\n",
    "\n",
    "Il metodo **forward** è responsabile dell'esecuzione dell'inoltro (forward pass) della rete neurale durante l'elaborazione dei dati. Prende in input il tensore x (input dell'immagine), il tempo t e l'etichetta y. t = t.unsqueeze(-1).type(torch.float): Aggiunge una dimensione all'input del tempo t all'ultimo indice utilizzando unsqueeze. Questo è fatto per garantire che il tensore del tempo abbia le stesse dimensioni del tensore di codifica di posizione restituito dal metodo pos_encoding. Successivamente, il tipo del tensore viene convertito in torch.float. Viene in seguito applicato il pos_encoding e si va a controllare.\n",
    "\n",
    "**if y is not None: t += self.label_emb(y):** Se l'etichetta y è disponibile (non è None), viene applicata un'embedding all'etichetta y utilizzando self.label_emb e poi sommata al tensore t. Questo permette di incorporare l'informazione dell'etichetta nel tensore t.\n",
    "\n",
    "Viene in seguito applicato il pattern di convoluzioni così come sono state definite e viene ritornato nell'ultima riga con **return self.exit(x4)** il risultatp, che equivale ad un tensore dopo l'applicazione dell'ultima convoluzione finale.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MyUNetConditioned(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256,num_classes=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.init = DoubleConv(c_in,64)\n",
    "\n",
    "        self.maxpool_conv1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(64, 64, residual=True),\n",
    "            DoubleConv(64, 128),\n",
    "        )\n",
    "        self.emb_layer1 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim,128),\n",
    "        )\n",
    "        self.sa1 = nn.Sequential(\n",
    "            SelfAttention(128,32)\n",
    "        )\n",
    "\n",
    "        self.maxpool_conv2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(128, 128, residual=True),\n",
    "            DoubleConv(128, 256),\n",
    "        )\n",
    "        self.emb_layer2 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim,256),\n",
    "        )\n",
    "        self.sa1_1 = nn.Sequential(\n",
    "            SelfAttention(256,16)\n",
    "        )\n",
    "\n",
    "        self.maxpool_conv2_1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(256, 256, residual=True),\n",
    "            DoubleConv(256, 256),\n",
    "        )\n",
    "        self.emb_layer2_1 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim,256),\n",
    "        )\n",
    "\n",
    "\n",
    "        #Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            DoubleConv(256, 512),\n",
    "            DoubleConv(512, 512),\n",
    "            DoubleConv(512, 256),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.up_conv3 = nn.Sequential(\n",
    "            DoubleConv(512, 512, residual=True),\n",
    "            DoubleConv(512, 128),\n",
    "        )\n",
    "        self.emb_layer3 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim,128),\n",
    "        )\n",
    "        self.sa2 = nn.Sequential(\n",
    "            SelfAttention(128,16)\n",
    "        )\n",
    "\n",
    "        self.up3_1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.up_conv3_1 = nn.Sequential(\n",
    "            DoubleConv(256, 256, residual=True),\n",
    "            DoubleConv(256, 64),\n",
    "        )\n",
    "        self.emb_layer3_1 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim,64),\n",
    "        )\n",
    "        self.sa1_2 = nn.Sequential(\n",
    "            SelfAttention(64,32)\n",
    "        )\n",
    "\n",
    "        self.up4 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.up_conv4 = nn.Sequential(\n",
    "            DoubleConv(128, 128, residual=True),\n",
    "            DoubleConv(128, 64),\n",
    "        )\n",
    "        self.emb_layer4 = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_dim,64),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.exit = nn.Conv2d(64,c_out,1)\n",
    "\n",
    "        if num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, time_dim)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels))\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t, y):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        if y is not None:\n",
    "            t += self.label_emb(y)\n",
    "\n",
    "        x = self.init(x)\n",
    "\n",
    "\n",
    "        x1 = self.maxpool_conv1(x)\n",
    "        emb1 = self.emb_layer1(t)[:, :, None, None].repeat(1, 1, x1.shape[-2], x1.shape[-1])\n",
    "        x1 = x1 + emb1\n",
    "        x1 = self.sa1(x1)\n",
    "\n",
    "\n",
    "        x2 = self.maxpool_conv2(x1)\n",
    "        emb2 = self.emb_layer2(t)[:, :, None, None].repeat(1, 1, x2.shape[-2], x2.shape[-1])\n",
    "        x2 = x2 + emb2\n",
    "        x2 = self.sa1_1(x2)\n",
    "\n",
    "\n",
    "        x2_1 = self.maxpool_conv2_1(x2)\n",
    "        emb2_1 = self.emb_layer2_1(t)[:, :, None, None].repeat(1, 1, x2_1.shape[-2], x2_1.shape[-1])\n",
    "        x2_1 = x2_1 + emb2_1\n",
    "\n",
    "        #Bottleneck\n",
    "        bottleneck = self.bottleneck(x2_1)\n",
    "\n",
    "\n",
    "        x3 = self.up3(bottleneck)\n",
    "        x3 = torch.cat((x3, x2), dim=1)\n",
    "        x3 = self.up_conv3(x3)\n",
    "        emb3 = self.emb_layer3(t)[:, :, None, None].repeat(1, 1, x3.shape[-2], x3.shape[-1])\n",
    "        x3 = x3 + emb3\n",
    "        x3 = self.sa2(x3)\n",
    "\n",
    "\n",
    "        x3_1 = self.up3_1(x3)\n",
    "        x3_1 = torch.cat((x3_1, x1), dim=1)\n",
    "        x3_1 = self.up_conv3_1(x3_1)\n",
    "        emb3_1 = self.emb_layer3_1(t)[:, :, None, None].repeat(1, 1, x3_1.shape[-2], x3_1.shape[-1])\n",
    "        x3_1 = x3_1 + emb3_1\n",
    "        x3_1 = self.sa1_2(x3_1)\n",
    "\n",
    "\n",
    "        x4 = self.up3_1(x3_1)\n",
    "        x4 = torch.cat((x4, x), dim=1)\n",
    "        x4 = self.up_conv4(x4)\n",
    "        emb4 = self.emb_layer4(t)[:, :, None, None].repeat(1, 1, x4.shape[-2], x4.shape[-1])\n",
    "        x4 = x4 + emb4\n",
    "\n",
    "\n",
    "        return self.exit(x4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![https://i.imgur.com/ip66Gk7.png](https://i.imgur.com/ip66Gk7.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La rete segue l’andamento mostrato nella figura. Come si può vedere si parte da una dimensione di 64x64 e si arriva ad una dimensione di 8x8. Inoltre, la rete parte prendendo in input 3 canali in ingresso (RGB) per arrivare a 512 nel passo di bottleneck e restituirne, infine, nuovamente 3.\n",
    "* In giallo sono mostrati i blocchi di SelfAttention applicati all’output delle singole convoluzioni\n",
    "* In blu sono mostrate le singole convoluzioni\n",
    "* In rosso sono mostrati i blocchi di maxpool, in verde quello di upsample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Loop <a id=\"training-loop\"></a>\n",
    "\n",
    "La funzione train definisce il ciclo di addestramento del modello. Prende in input diversi parametri come run_name (nome dell'esecuzione), epochs (numero di epoche di addestramento), image_size (dimensione delle immagini), device (dispositivo di calcolo), lr (learning rate), num_classes (numero di classi), checkpoint (per riprendere l'addestramento da un checkpoint salvato in precedenza).\n",
    "\n",
    "Ecco come funziona la funzione train:\n",
    "\n",
    "* Viene inizializzato il dispositivo di calcolo device utilizzando il valore fornito come input.\n",
    "* Viene creato un oggetto dataloader utilizzando il modulo loader (presumibilmente un oggetto DataLoader che carica i dati per l'addestramento).\n",
    "* Viene istanziato il modello MyUNetConditioned specificando il numero di classi num_classes. Il modello viene spostato sul dispositivo di calcolo utilizzando il metodo to(device).\n",
    "* Se è specificato un checkpoint (checkpoint=True), il modello viene caricato utilizzando model.load_state_dict(torch.load(checkpoint)).\n",
    "* Viene istanziato l'ottimizzatore AdamW utilizzando i parametri del modello e il learning rate lr.\n",
    "* Viene definita la loss function mse (Mean Squared Error) utilizzando nn.MSELoss().\n",
    "* Viene creato un oggetto diffusion utilizzando la classe Diffusion, specificando la dimensione delle immagini image_size e il dispositivo di calcolo device.\n",
    "* Viene istanziato un oggetto EMA (Exponential Moving Average) utilizzando un fattore di decadimento 0.995.\n",
    "* Viene creato un modello ema_model utilizzando la funzione copy.deepcopy(model). Il modello viene impostato come valutazione (eval()) e non richiede il calcolo del gradiente (requires_grad_(False)).\n",
    "* Inizia il ciclo di addestramento delle epoche. Viene iterato su ciascuna epoca.\n",
    "* Viene creato un oggetto tqdm per visualizzare la barra di avanzamento del training.\n",
    "* Viene iterato su ogni batch di dati nel dataloader.\n",
    "* Le immagini e le etichette vengono spostate sul dispositivo di calcolo utilizzando images.to(device) e labels.to(device).\n",
    "* Viene campionato un tensore di timesteps t utilizzando il metodo sample_timesteps dell'oggetto diffusion.\n",
    "* Vengono generati x_t (immagini con rumore) e noise utilizzando il metodo noise_images dell'oggetto diffusion passando le immagini di input images e il tensore dei tempi t.\n",
    "* Se un numero casuale generato è inferiore a 0.1, l'etichetta labels viene impostata come None.\n",
    "* Viene calcolato il rumore predetto utilizzando il modello model applicando x_t, t e labels.\n",
    "* Viene calcolata la loss function confrontando il rumore predetto con il rumore reale, utilizzando la funzione mse(noise, predicted_noise).\n",
    "* Vengono azzerati i gradienti degli ottimizzatori chiamando optimizer.zero_grad().\n",
    "* Viene calcolato il gradiente della loss function chiamando loss.backward().\n",
    "* Vengono eseguiti i passaggi di ottimizzazione chiamando optimizer.step().\n",
    "* Viene eseguito il passo EMA (Exponential Moving Average) chiamando ema.step_ema(ema_model, model).\n",
    "* Vengono aggiornate le informazioni sulla barra di avanzamento chiamando pbar.set_postfix(MSE=loss.item()).\n",
    "* Se l'epoca è un multiplo di 100, vengono campionate immagini utilizzando il modello model e l'oggetto diffusion passando il numero di classi num_classes e le immagini vengono salvate in una cartella specifica.\n",
    "* Vengono salvati i checkpoint del modello ema_model e model in file separati.\n",
    "\n",
    "Il ciclo di addestramento termina dopo tutte le epoche specificate che nel caso attuale sono 500."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train(run_name = \"DDPM_Condtional\",epochs = 500,image_size = 64, device = \"cuda\",lr = 3e-4,num_classes = 22,checkpoint=False):\n",
    "    device = device\n",
    "    dataloader = loader\n",
    "    model = MyUNetConditioned(num_classes=num_classes).to(device)\n",
    "        \n",
    "    if checkpoint:\n",
    "        model.load_state_dict(torch.load(checkpoint))\n",
    "        \n",
    "        print(\"Loaded checkpoint\")\n",
    "        \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=image_size, device=device)\n",
    "    #logger = SummaryWriter(os.path.join(\"runs\", run_name))\n",
    "    l = len(dataloader)\n",
    "    ema = EMA(0.995)\n",
    "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #logging.info(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(dataloader)\n",
    "        print(\"Epoch number: \", epoch+1)\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            if np.random.random() < 0.1:\n",
    "                labels = None\n",
    "            predicted_noise = model(x_t, t, labels)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema.step_ema(ema_model, model)\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "            #logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "        if (epoch + 1) % 70 == 0:\n",
    "            labels = torch.arange(num_classes).long().to(device)\n",
    "            #sampled_images = diffusion.sample(model, n=len(labels), labels=labels)\n",
    "            ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n",
    "            #plot_images(sampled_images,22)\n",
    "            #plot_images(ema_sampled_images, 8)\n",
    "            #save_images(sampled_images, os.path.join(\"results\", \"/kaggle/working/\", f\"{epoch}.jpg\"))\n",
    "            save_images(ema_sampled_images, os.path.join(\"results_ema\", \"/kaggle/working/\", f\"{epoch+1}.jpg\"))\n",
    "        torch.save(ema_model.state_dict(), os.path.join(\"models\", \"/kaggle/working/\", f\"ema_ckpt.pt\"))\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", \"/kaggle/working/\", f\"ckpt.pt\"))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.315080Z",
     "iopub.execute_input": "2023-06-22T11:49:08.315404Z",
     "iopub.status.idle": "2023-06-22T11:49:08.329362Z",
     "shell.execute_reply.started": "2023-06-22T11:49:08.315349Z",
     "shell.execute_reply": "2023-06-22T11:49:08.328259Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main <a id=\"main\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = MyUNetConditioned(num_classes = 22)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(\"Total number of parameters: \", total_params)\n",
    "\n",
    "train(run_name = \"DDPM_Uncondtional\",\n",
    "    epochs = 100,\n",
    "    image_size = 64,\n",
    "    #dataset_path = r\"C:\\Users\\dome\\datasets\\landscape_img_folder\",\n",
    "    device = \"cuda\",\n",
    "    lr = 3e-4,\n",
    "    checkpoint=r\"/kaggle/input/ema-ckpt-tuning/ema_ckpt.pt\"\n",
    "     )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-22T11:49:08.330573Z",
     "iopub.execute_input": "2023-06-22T11:49:08.331249Z"
    },
    "trusted": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
